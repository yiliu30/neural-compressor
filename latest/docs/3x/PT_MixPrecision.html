<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>PyTorch Mixed Precision &mdash; Intel® Neural Compressor 3.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "neural-compressor"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Intel® Neural Compressor
          </a>
            <div class="version">
              <a href="../../../versions.html">latest▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../source/get_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/installation_guide.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/user_guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/examples_readme.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/api-doc/apis.html">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/releases_info.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/legal_information.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/SECURITY.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/neural-compressor">Repo</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Intel® Neural Compressor</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">PyTorch Mixed Precision</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/docs/3x/PT_MixPrecision.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="pytorch-mixed-precision">
<h1>PyTorch Mixed Precision<a class="headerlink" href="#pytorch-mixed-precision" title="Permalink to this heading"></a></h1>
<ol class="simple">
<li><p><a class="reference external" href="#introduction">Introduction</a></p></li>
<li><p><a class="reference external" href="#mixed-precision-support-matrix">Mixed Precision Support Matrix</a></p></li>
<li><p><a class="reference external" href="#get-start">Get Started</a></p></li>
<li><p><a class="reference external" href="#examples">Examples</a></p></li>
</ol>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading"></a></h2>
<p>The recent growth of Deep Learning has driven the development of more complex models that require significantly more compute and memory capabilities. Several low precision numeric formats have been proposed to address the problem. Google’s <a class="reference external" href="https://cloud.google.com/tpu/docs/bfloat16">bfloat16</a> and the <a class="reference external" href="https://en.wikipedia.org/wiki/Half-precision_floating-point_format">FP16: IEEE</a> half-precision format are two of the most widely used sixteen bit formats. <a class="reference external" href="https://arxiv.org/abs/1710.03740">Mixed precision</a> training and inference using low precision formats have been developed to reduce compute and bandwidth requirements.</p>
<p>The 3rd Gen Intel® Xeon® Scalable processor (codenamed Cooper Lake), featuring Intel® Deep Learning Boost, is the first general-purpose x86 CPU to support the bfloat16 format. Specifically, three new bfloat16 instructions are added as a part of the AVX512_BF16 extension within Intel Deep Learning Boost: VCVTNE2PS2BF16, VCVTNEPS2BF16, and VDPBF16PS. The first two instructions allow converting to and from bfloat16 data type, while the last one performs a dot product of bfloat16 pairs. Further details can be found in the <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/articles/technical/intel-deep-learning-boost-new-instruction-bfloat16.html">hardware numerics document</a> published by Intel.</p>
<p>The 4th Gen Intel® Xeon® Scalable processor supports FP16 instruction set architecture (ISA) for Intel®
Advanced Vector Extensions 512 (Intel® AVX-512). The new ISA supports a wide range of general-purpose numeric
operations for 16-bit half-precision IEEE-754 floating-point and complements the existing 32-bit and 64-bit floating-point instructions already available in the Intel Xeon processor based products. Further details can be found in the <a class="reference external" href="https://www.intel.com/content/www/us/en/content-details/669773/intel-avx-512-fp16-instruction-set-for-intel-xeon-processor-based-products-technology-guide.html">hardware numerics document</a> published by Intel.</p>
<p align="center" width="100%">
    <img src="./imgs/data_format.png" alt="Architecture" height=230>
</p></section>
<section id="mixed-precision-support-matrix">
<h2>Mixed Precision Support Matrix<a class="headerlink" href="#mixed-precision-support-matrix" title="Permalink to this heading"></a></h2>
<table class="center">
    <thead>
        <tr>
            <th>Framework</th>
            <th>Backend</th>
            <th>Backend Library</th>
            <th>Backend Value</th>
            <th>Support Device(cpu as default)</th> 
            <th>Support BF16</th>
            <th>Support FP16</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td rowspan="1" align="left">PyTorch</td>
            <td align="left">FX</td>
            <td align="left">FBGEMM</td>
            <td align="left">"default"</td>
            <td align="left">cpu</td>
            <td align="left">&#10004;</td>
            <td align="left">&#10004;</td>
        </tr>
    </tbody>
</table><section id="hardware-and-software-requests-for-bf16">
<h3>Hardware and Software requests for <strong>BF16</strong><a class="headerlink" href="#hardware-and-software-requests-for-bf16" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>PyTorch</p>
<ol class="simple">
<li><p>Hardware: CPU supports <code class="docutils literal notranslate"><span class="pre">avx512_bf16</span></code> instruction set.</p></li>
<li><p>Software: torch &gt;= <a class="reference external" href="https://download.pytorch.org/whl/torch_stable.html">1.11.0</a>.</p></li>
</ol>
</li>
</ul>
</section>
<section id="hardware-and-software-requests-for-fp16">
<h3>Hardware and Software requests for <strong>FP16</strong><a class="headerlink" href="#hardware-and-software-requests-for-fp16" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>PyTorch</p>
<ol class="simple">
<li><p>Hardware: CPU supports <code class="docutils literal notranslate"><span class="pre">avx512_fp16</span></code> instruction set.</p></li>
<li><p>Software: torch &gt;= <a class="reference external" href="https://download.pytorch.org/whl/torch_stable.html">1.11.0</a>.</p></li>
</ol>
</li>
</ul>
</section>
<section id="accuracy-driven-mixed-precision">
<h3>Accuracy-driven mixed precision<a class="headerlink" href="#accuracy-driven-mixed-precision" title="Permalink to this heading"></a></h3>
<p>BF16/FP16 conversion may lead to accuracy drop. Intel® Neural Compressor provides an accuracy-driven tuning function to reduce accuracy loss,
which could fallback converted ops to FP32, if set in config, to get better accuracy. To enable this function, users only to provide
<code class="docutils literal notranslate"><span class="pre">eval_fn</span></code> and <code class="docutils literal notranslate"><span class="pre">eval_args</span></code> for <code class="docutils literal notranslate"><span class="pre">autotune</span></code>.<br />To be noticed, IPEX backend doesn’t support accuracy-driven mixed precision.</p>
</section>
</section>
<section id="get-started-with-autotune-api">
<h2>Get Started with autotune API<a class="headerlink" href="#get-started-with-autotune-api" title="Permalink to this heading"></a></h2>
<p>To get a bf16/fp16 model, users can use the <code class="docutils literal notranslate"><span class="pre">autotune</span></code> interface with <code class="docutils literal notranslate"><span class="pre">MixPrecisionConfig</span></code> as follows.</p>
<ul class="simple">
<li><p>BF16:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">neural_compressor.torch.quantization</span> <span class="kn">import</span> <span class="n">MixPrecisionConfig</span><span class="p">,</span> <span class="n">TuningConfig</span><span class="p">,</span> <span class="n">autotune</span>

<span class="k">def</span> <span class="nf">eval_acc_fn</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="o">......</span>
    <span class="k">return</span> <span class="n">acc</span>

<span class="c1"># modules might be fallback to fp32 to get better accuracy</span>
<span class="n">custom_tune_config</span> <span class="o">=</span> <span class="n">TuningConfig</span><span class="p">(</span><span class="n">config_set</span><span class="o">=</span><span class="p">[</span><span class="n">MixPrecisionConfig</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;bf16&quot;</span><span class="p">,</span> <span class="s2">&quot;fp32&quot;</span><span class="p">])],</span> <span class="n">max_trials</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">best_model</span> <span class="o">=</span> <span class="n">autotune</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">build_torch_model</span><span class="p">(),</span> <span class="n">tune_config</span><span class="o">=</span><span class="n">custom_tune_config</span><span class="p">,</span> <span class="n">eval_fn</span><span class="o">=</span><span class="n">eval_acc_fn</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>FP16:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">neural_compressor.torch.quantization</span> <span class="kn">import</span> <span class="n">MixPrecisionConfig</span><span class="p">,</span> <span class="n">TuningConfig</span><span class="p">,</span> <span class="n">autotune</span>

<span class="k">def</span> <span class="nf">eval_acc_fn</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="o">......</span>
    <span class="k">return</span> <span class="n">acc</span>

<span class="c1"># modules might be fallback to fp32 to get better accuracy</span>
<span class="n">custom_tune_config</span> <span class="o">=</span> <span class="n">TuningConfig</span><span class="p">(</span><span class="n">config_set</span><span class="o">=</span><span class="p">[</span><span class="n">MixPrecisionConfig</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;fp16&quot;</span><span class="p">,</span> <span class="s2">&quot;fp32&quot;</span><span class="p">])],</span> <span class="n">max_trials</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">best_model</span> <span class="o">=</span> <span class="n">autotune</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">build_torch_model</span><span class="p">(),</span> <span class="n">tune_config</span><span class="o">=</span><span class="n">custom_tune_config</span><span class="p">,</span> <span class="n">eval_fn</span><span class="o">=</span><span class="n">eval_acc_fn</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Permalink to this heading"></a></h2>
<p>Example will be added later.</p>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Neural Compressor, Intel.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7efd22b77730> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>